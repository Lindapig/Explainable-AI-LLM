{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6652b72-2c3d-482a-86e3-89b4634c0e45",
   "metadata": {},
   "source": [
    "## Programming Assignment 4 - Building Ethical & Explainable AI\n",
    "\n",
    "In this module, you will take on the role of an AI researcher at a technology startup tasked with developing a predictive tool for a financial institution. The client has identified that accurate income prediction could provide valuable insights for various business functions, such as identifying clients eligible for premium services or targeting financial assistance programs. However, it is essential that this tool is both ethical and interpretable, ensuring that decisions do not perpetuate societal biases or discriminate against individuals based on sensitive attributes.\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Using the [Adult dataset](https://archive.ics.uci.edu/dataset/2/adult), you will develop a machine learning solution to predict whether an individual’s income is above or below $50K, based on features such as age, occupation, education, and hours worked. The model should not only achieve high accuracy in income prediction but also demonstrate transparency in its predictions and minimize any unfair biases related to sensitive attributes, including gender, race, and age.\n",
    "\n",
    "### Overview of Tasks:\n",
    "\n",
    "1. **Data Exploration and Preprocessing:** Start with a thorough exploratory data analysis (EDA) and basic dataset cleaning. Document your observations from the EDA, noting any patterns or potential biases.\n",
    "\n",
    "2. **Model Development:** Using insights from previous assignments, build a Multi-Layer Perceptron (MLP) Classifier to predict income categories.\n",
    "\n",
    "3. **Explainability Analysis:** Use the SHAP library to analyze feature importance and explain model predictions.\n",
    "\n",
    "4. **Fairness Evaluation:** Assess the model using fairness metrics, including Statistical Parity Difference, Disparate Impact, and Equal Opportunity Difference. Evaluate model performance across various demographic groups, identifying any disparities or biases.\n",
    "\n",
    "5. **Bias Mitigation:** Implement an in-processing bias mitigation technique (Adversarial Debiasing) to improve model fairness. Re-evaluate the model post-mitigation to assess both fairness improvements and predictive performance retention.\n",
    "\n",
    "6. **Ethical Reflection:** Document your methodology, findings, and reflections. Provide an analysis of the ethical implications of your model’s predictions, especially in relation to sensitive demographic groups.\n",
    "\n",
    "This notebook requires students to do some digging on their own; you will need to refer to cited research papers, library documentations and YouTube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca1567-39c8-4a80-a608-5362387cac84",
   "metadata": {},
   "source": [
    "#### 1. Installing the required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18048156-e7fd-412c-b8a0-5f04d756867a",
   "metadata": {},
   "source": [
    "We need to install the following dependencies inorder to run the notebook file. Make sure you create virtual environment using a dependency manager of your choice. Activate the virtual environment and then uncomment & run the following line of code. If you are unsure of the procedure to create a virtual environment, follow the steps provided in this [link](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a993a7-4934-48ac-9ffe-60f621b056a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install scikit-learn pandas numpy matplotlib seaborn fairlearn ucimlrepo shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a238bac-e047-4d08-a3aa-a2c5102094ad",
   "metadata": {},
   "source": [
    "We will also be using PyTorch for building Neural Networks. Follow the instructions given in the [official PyTorch documentation](https://pytorch.org/get-started/locally/) to install it in your machine based on your OS and architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a50d8-3a3a-425a-bfb3-7645dab9decd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PyTorch CPU is simpler to install across different OS and architecture and\n",
    "# is more than sufficient for this question.\n",
    "\n",
    "# For installing PyTorch CPU, you can simply uncomment and run the line below.\n",
    "\n",
    "# !pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15499c-1893-4b9d-be3a-5b648e9e1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import shap\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from fairlearn.adversarial import AdversarialFairnessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68d5d2-46ec-460f-9470-69da38d10578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the libraries for reproducibility\n",
    "\n",
    "# Don't change the seed value\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd8c32-87c6-4f57-9d5d-49230a5f4f05",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10a94f-174a-4886-9311-2c405cb21c57",
   "metadata": {},
   "source": [
    "#### Dataset Description:\n",
    "\n",
    "The Adult dataset, also known as the \"Census Income\" dataset, is a widely used dataset in the UCI Machine Learning Repository. It is primarily used for binary classification tasks, where the goal is to predict whether an individual's income exceeds $50K annually based on their demographic and employment information. The description of the various features present in the dataset is tabulated below.\n",
    "\n",
    "\n",
    "| **Feature**         | **Type**               | **Description**                                                                                  |\n",
    "|---------------------|------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| `age`               | Numerical              | The age of the individual.                                                                       |\n",
    "| `workclass`         | Categorical            | The type of employer (e.g., Private, Self-emp, Government, etc.).                                |\n",
    "| `fnlwgt`            | Numerical              | Final weight, representing the number of people the census believes this entry represents.       |\n",
    "| `education`         | Categorical            | The highest level of education attained (e.g., Bachelors, HS-grad, etc.).                        |\n",
    "| `education-num`     | Numerical              | The number of years of education.                                                                |\n",
    "| `marital-status`    | Categorical            | Marital status (e.g., Married, Never-married, etc.).                                             |\n",
    "| `occupation`        | Categorical            | The type of occupation (e.g., Tech-support, Sales, etc.).                                        |\n",
    "| `relationship`      | Categorical            | Relationship to household head (e.g., Husband, Wife, Own-child, etc.).                           |\n",
    "| `race`              | Categorical            | Race of the individual (e.g., White, Asian-Pac-Islander, Black, etc.).                           |\n",
    "| `sex`               | Categorical            | Gender of the individual (e.g., Male, Female).                                                   |\n",
    "| `capital-gain`      | Numerical              | Capital gains earned by the individual.                                                          |\n",
    "| `capital-loss`      | Numerical              | Capital losses incurred by the individual.                                                       |\n",
    "| `hours-per-week`    | Numerical              | The number of hours worked per week.                                                             |\n",
    "| `native-country`    | Categorical            | Country of origin (e.g., United-States, Mexico, etc.).                                           |\n",
    "| `income`            | Categorical (Label)    | Income class label (<=50K or >50K).                                                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27276b-cdc0-494d-887e-a359963e18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the Adult dataset from the UCI Machine Learning repository.\n",
    "# Don't modify this code cell\n",
    "\n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets  \n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ab554-98bd-47c9-b29c-4f7bbdc4067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf6b6a-e497-43ec-83b5-34a85493ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071d059-de75-4d0c-ae97-5cd1231d0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a94cb-3b91-48eb-99e1-2f6a3bf4a4ab",
   "metadata": {},
   "source": [
    "**Remove the Null Values from the dataset.**  [Pandas Docs](https://pandas.pydata.org/docs/user_guide/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b7d7e-ef73-4c63-95e8-f246f0599d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b992af-430c-4a1e-87a8-ebd80ca6eac6",
   "metadata": {},
   "source": [
    "**Check if any of the categorical features has unknown categories and remove them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435f8d1-0466-417e-87f5-844319a875f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b821fd-b90b-4101-a489-926e25a68cb0",
   "metadata": {},
   "source": [
    "**Check if the dataset consists of duplicate values and remove them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097198e2-b15d-4cca-bb11-8f763cff8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043dd7ae-9e95-4579-9f68-b5f68a6db2fd",
   "metadata": {},
   "source": [
    "**The occupation feature has high cardinality. Let's group similar occupations together based on the given mapping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f7ae7-634c-4f47-a9ed-7012d636b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_groups = {\n",
    "    'Craft-repair': 'Skilled Labor',\n",
    "    'Machine-op-inspct': 'Skilled Labor',\n",
    "    'Farming-fishing': 'Skilled Labor',\n",
    "    'Transport-moving': 'Skilled Labor',\n",
    "    'Handlers-cleaners': 'Skilled Labor',\n",
    "    \n",
    "    'Prof-specialty': 'Professional',\n",
    "    'Exec-managerial': 'Professional',\n",
    "    \n",
    "    'Adm-clerical': 'Administrative/Clerical',\n",
    "    'Tech-support': 'Administrative/Clerical',\n",
    "    \n",
    "    'Sales': 'Sales',\n",
    "    \n",
    "    'Other-service': 'Service',\n",
    "    'Protective-serv': 'Service',\n",
    "    'Priv-house-serv': 'Service',\n",
    "    \n",
    "    'Armed-Forces': 'Military'\n",
    "}\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "df_cleaned[\"occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d508c9f-f654-42df-a1da-7888d6cd4551",
   "metadata": {},
   "source": [
    "**Also group the education categories together based on the given mapping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a16220-5767-4029-92bd-2445c19d23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_groups = {\n",
    "    'Preschool': 'Preschool',\n",
    "    '1st-4th': 'Primary School',\n",
    "    '5th-6th': 'Primary School',\n",
    "    '7th-8th': 'Middle School',\n",
    "    '9th': 'High School (Incomplete)',\n",
    "    '10th': 'High School (Incomplete)',\n",
    "    '11th': 'High School (Incomplete)',\n",
    "    '12th': 'High School (Incomplete)',\n",
    "    'HS-grad': 'High School Graduate',\n",
    "    'Some-college': 'Some College/Associate Degree',\n",
    "    'Assoc-voc': 'Some College/Associate Degree',\n",
    "    'Assoc-acdm': 'Some College/Associate Degree',\n",
    "    'Bachelors': \"Bachelor's Degree\",\n",
    "    'Masters': 'Graduate School',\n",
    "    'Prof-school': 'Graduate School',\n",
    "    'Doctorate': 'Graduate School'\n",
    "}\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "df_cleaned[\"education\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ca7db-e0b4-42ac-9915-e623a6cee494",
   "metadata": {},
   "source": [
    "**Group the similar workclass categories together based on the given mapping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04895298-667d-480b-afef-0ba093b4dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "workclass_groups = {\n",
    "    'Private': 'Private Sector',\n",
    "    'Self-emp-not-inc': 'Self-employed',\n",
    "    'Self-emp-inc': 'Self-employed',\n",
    "    'Local-gov': 'Government',\n",
    "    'State-gov': 'Government',\n",
    "    'Federal-gov': 'Government',\n",
    "    'Without-pay': 'Unpaid'\n",
    "}\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "df_cleaned[\"workclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78357d2d-63d9-4ca7-954c-7d68b2d8b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "df_cleaned['marital-status'] = df_cleaned['marital-status'].replace(\n",
    "    {'Married-civ-spouse': 'Married', 'Married-spouse-absent': 'Married', 'Married-AF-spouse': 'Married'}\n",
    ")\n",
    "\n",
    "df_cleaned[\"marital-status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0cec91-0aef-4b65-a498-3fe5aa8393ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "df_cleaned['race'] = df_cleaned['race'].replace({\n",
    "    'Asian-Pac-Islander': 'Other',\n",
    "    'Amer-Indian-Eskimo': 'Other'\n",
    "})\n",
    "\n",
    "df_cleaned[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c2f85-152f-4a4b-af7f-3934055ca6a9",
   "metadata": {},
   "source": [
    "**Check if the target variable (income) is in the required format. If not preprocess it accordingly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97440ca6-cff8-4116-b015-3f50c5c31170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12638db-f752-4404-9334-d40e1cc6a948",
   "metadata": {},
   "source": [
    "For the sake of simplicity, let's drop the `education-num`, `relationship`, `fnlwgt` and `native-country` columns.\n",
    "\n",
    "**Write code for dropping the mentioned columns from the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037592ab-565d-49b3-9afb-796db5a0acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d065b7-48a0-4b41-9551-962dcef34c87",
   "metadata": {},
   "source": [
    "We observe that the dataset now includes 10 features and 1 target variable (`income`):\n",
    "\n",
    "- `age`\n",
    "- `workclass`\n",
    "- `education`\n",
    "- `marital-status`\n",
    "- `occupation`\n",
    "- `race`\n",
    "- `sex`\n",
    "- `capital-gain`\n",
    "- `capital-loss`\n",
    "- `hours-per-week`\n",
    "- `income`\n",
    "\n",
    "Suppose there exists an optimal model, $m^*$, which performs well in terms of both performance and fairness evaluation.\n",
    "\n",
    "**Formulate a hypothesis about how the model $m^*$ should make predictions based on the input variables. Specifically, consider which variables the model should prioritize or weigh more heavily and which it might give less attention in our case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a45043-8561-499a-9ffd-3e53e07cc24b",
   "metadata": {},
   "source": [
    "Write you answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df329ab-5e79-487c-be1f-237d8b949510",
   "metadata": {},
   "source": [
    "### Introduction to Sensitive Features and Bias in Machine Learning Models\n",
    "\n",
    "In machine learning, **sensitive features** are attributes related to personal characteristics that should not be unfairly used to influence the predictions of a model. These features often include variables like **gender, race, age, disability status, or religion**. Because these attributes are closely tied to identity, they risk introducing **social biases** into machine learning models if not handled carefully. When sensitive features are present, models may inadvertently learn patterns that reinforce societal inequalities, resulting in unfair or discriminatory outcomes for certain groups.\n",
    "\n",
    "#### Why Sensitive Features Introduce Bias\n",
    "\n",
    "Sensitive features introduce bias because they are often correlated with target outcomes and other features in ways that reflect historical inequalities or discrimination. For example, race and income may be correlated because of systemic disparities in socioeconomic opportunities. Without careful handling, a machine learning model may inadvertently capture and amplify these correlations, leading to **disparate impacts** on different demographic groups. This can result in **unintended discrimination** where the model’s predictions favor one group over another.\n",
    "\n",
    "#### Why Simply Removing Sensitive Features Doesn’t Eliminate Bias\n",
    "\n",
    "It might seem like a straightforward solution to remove sensitive features from the dataset, under the assumption that this will make the model \"blind\" to these attributes. However, **removing sensitive features alone is insufficient** for achieving fairness, because:\n",
    "\n",
    "1. **Indirect Correlations**: Other features, known as **proxy variables**, may carry information about sensitive attributes. For example, ZIP codes or education levels might be indirectly correlated with race or socioeconomic status.\n",
    "2. **Data Imbalances**: The data itself may be imbalanced in terms of representation. Even without explicit sensitive features, models trained on biased data are likely to perform poorly for underrepresented groups.\n",
    "3. **Historical Biases**: Sensitive attributes often reflect broader historical biases, so the outcomes themselves (e.g., income levels) are biased. Removing sensitive features without addressing these biases still leaves the model with skewed patterns learned from the data.\n",
    "\n",
    "#### Strategies for Mitigating Bias in Machine Learning Models\n",
    "\n",
    "Bias mitigation in machine learning involves applying specific techniques to ensure that model outcomes are fair across different demographic groups. Bias mitigation strategies can generally be divided into three categories:\n",
    "\n",
    "1. **Pre-processing**: Transforming the data before training so that the sensitive attributes do not introduce unfair bias into the model. This can involve re-sampling, re-weighting, or modifying the dataset to balance representation across groups.\n",
    "   \n",
    "2. **In-processing**: Incorporating fairness constraints or adjustments into the model during training. For instance, **Adversarial Debiasing** is an in-processing technique that uses an adversarial component to reduce the influence of sensitive features on the predictions.\n",
    "\n",
    "3. **Post-processing**: Adjusting the model’s predictions after training to ensure fairer outcomes. Post-processing can help align the model's results with fairness metrics, ensuring consistent performance across demographic groups.\n",
    "\n",
    "Each of these strategies has its advantages and limitations, and often, a combination of these methods is required to achieve fair and unbiased outcomes.\n",
    "\n",
    "For this module, we will be using the Adversarial Debiasing technique, which uses an adversarial network to debias the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fde02-9ba6-4b0e-a32c-a9cad4597b1f",
   "metadata": {},
   "source": [
    "**Based on your understanding of the problem statement, which of the attributes from our dataset could be sensistive variables? Provide reasoning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35633032-7d3d-4d0b-9a2b-8c5b87656bf7",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis\n",
    "\n",
    "Let us visualize some of the sensitive attributes in the dataset and try to find interesting observations from the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d129f-4183-4809-997b-c727695b4701",
   "metadata": {},
   "source": [
    "**Visualize the distribution of the sex feature across both the income categories. Write down your observations about the age feature from the visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5853753-5b23-428a-b36c-8b2420a882b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2de2af-9a29-4d87-ae35-39c27f8f51aa",
   "metadata": {},
   "source": [
    "Write you observations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68be52-ba9a-419a-b1d1-63ebdf1605b5",
   "metadata": {},
   "source": [
    "**Visualize the distribution of the marital status feature across both the income categories. Write down your observations about the marital status feature from the visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0e10c-d21d-47f3-8ba7-272d315f9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afec3a-a5ad-40f6-a957-849ea7268c8a",
   "metadata": {},
   "source": [
    "Write you observations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5f5ee-c38a-4e94-b278-fd72eed08466",
   "metadata": {},
   "source": [
    "**Visualize the distribution of the race feature across both the income categories. Write down your observations about the race feature from the visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa32ac-d9a8-4c18-b893-6079e340f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42cb104-d6fa-40f0-9a00-a01ba84c1b92",
   "metadata": {},
   "source": [
    "Write you observations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf72b8d-549c-4c46-813a-91839f174050",
   "metadata": {},
   "source": [
    "**Data Encoding for ML Models Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3714b8-2660-42b5-a2fa-96d1e8bd460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning models requires the categorical features to be encoded\n",
    "# in specific formats such as Ordinal or One-Hot encoded. The following line of\n",
    "# code does Ordinal Encoding of the categorical column present in the dataset\n",
    "# Check the utils file to see how the categories are encoded.\n",
    "\n",
    "from utils import encode_df_for_ml_models\n",
    "\n",
    "df_encoded = encode_df_for_ml_models(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c5194-abe7-4414-9d17-da1d5453a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_label_encoding = {\n",
    "    \"<=50K\": 0,\n",
    "    \">50K\": 1\n",
    "}\n",
    "\n",
    "df_encoded['income'] = df_encoded['income'].map(income_label_encoding)\n",
    "\n",
    "df_encoded[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a85430-b49c-4dfe-ad93-97416dc75799",
   "metadata": {},
   "source": [
    "#### Introduction to Fairness Metrics in Machine Learning\n",
    "\n",
    "Fairness metrics are designed to evaluate whether a machine learning model treats different demographic groups (e.g., based on gender, race, age) equitably in its predictions. These metrics are crucial for identifying and addressing any unintended biases that may result from training data or the model's learning process. Below are three commonly used fairness metrics that students will implement and analyze for their models:\n",
    "\n",
    "##### Disparate Impact\n",
    "\n",
    "The measure of disparate impact is formulated to represent the legal concept of \"disparate impact.\" It ensures a balanced ratio between the positive prediction rates across different groups, meaning that the proportion of positive predictions should be consistent between groups. For instance, if a positive prediction corresponds to admission into a training program, this condition aims to ensure similar admission rates across groups. The measure is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{P(\\hat{Y} = 1 \\mid S \\neq 1)}{P(\\hat{Y} = 1 \\mid S = 1)} \\geq 1 - \\epsilon\n",
    "$$\n",
    "\n",
    "where $S$ is the protected attribute (such as race or gender), $S = 1$ represents the privileged group, and $S \\neq 1$ denotes the unprivileged group. Here, $\\hat{Y} = 1$ indicates a positive prediction. When $\\hat{Y} = 1$ represents admission (e.g., to a training program), this condition seeks to ensure similar admission rates for both groups. A higher value for this measure implies more balanced rates between groups, indicating greater fairness. This concept aligns with the “80 percent rule” in disparate impact law, which mandates that the admission rate for any demographic group should be at least 80% of that of the group with the highest admission rate.\n",
    "\n",
    "##### Demographic / Statistical parity\n",
    "\n",
    "Demographic parity, also known as statistical parity, is similar to disparate impact, but instead of using a ratio, it evaluates the difference in positive prediction rates between groups. This measure ensures that the positive prediction rates are similar across groups. Formally, demographic parity is defined as:\n",
    "\n",
    "$$\n",
    "\\left| P(\\hat{Y} = 1 \\mid S = 1) - P(\\hat{Y} = 1 \\mid S \\neq 1) \\right| \\leq \\epsilon\n",
    "$$\n",
    "\n",
    "where $S$ is the protected attribute (such as race or gender), $S = 1$ represents the privileged group, and $S \\neq 1$ denotes the unprivileged group. A lower value of this measure implies more similar rates of positive predictions across groups, indicating greater fairness. Both demographic parity and disparate impact aim to assign positive predictions at similar rates for the two groups.\n",
    "\n",
    "However, one limitation of these measures is that they may classify a highly accurate model as unfair if the base rates (the actual proportion of positive outcomes) differ significantly between groups. Additionally, to satisfy demographic parity, two similar individuals might receive different treatment based on group membership, which may be legally prohibited in some cases. This concept is also associated with affirmative action practices.\n",
    "\n",
    "##### Equalized Odds\n",
    "\n",
    "Equalized odds, a measure introduced by Hardt et al., aims to address the limitations of measures such as disparate impact and demographic parity. This measure evaluates fairness by comparing the false-positive rates (FPRs) and true-positive rates (TPRs) across groups, ensuring that both are similar between groups. Formally, equalized odds is defined by the following two conditions:\n",
    "\n",
    "$$\n",
    "\\left| P(\\hat{Y} = 1 \\mid S = 1, Y = 0) - P(\\hat{Y} = 1 \\mid S \\neq 1, Y = 0) \\right| \\leq \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left| P(\\hat{Y} = 1 \\mid S = 1, Y = 1) - P(\\hat{Y} = 1 \\mid S \\neq 1, Y = 1) \\right| \\leq \\epsilon\n",
    "$$\n",
    "\n",
    "where the first condition requires the absolute difference in the FPRs of the two groups to be bounded by $ \\epsilon $, and the second condition requires the absolute difference in the TPRs of the two groups to be similarly bounded. Smaller differences indicate better fairness.\n",
    "\n",
    "Unlike demographic parity and disparate impact, an entirely accurate classifier will always satisfy the equalized odds criteria. However, equalized odds relies on the actual ground truth labels ($ Y $), which assumes that the base rates of the groups are representative and free from bias.\n",
    "\n",
    "One notable use case demonstrating the effectiveness of this measure is the investigation of the COMPAS algorithm used in the U.S. criminal justice system. Although COMPAS had similar accuracy for both African-American and Caucasian groups when predicting recidivism, the odds differed. The algorithm falsely predicted future criminality (FPR) for African-Americans at twice the rate compared to Caucasians. Additionally, the algorithm significantly underestimated future crimes among Caucasians, resulting in a higher false-negative rate (FNR) for this group.\n",
    "\n",
    "You can refer to this awesome Youtube Playlist on [Algorithmic Fairness](https://youtube.com/playlist?list=PLqDyyww9y-1Q0zWbng6vUOG1p3oReE2xS&si=fW59id6qAsIPwwOA) to understand the metrics and Algorithmic Fairness domain in more detail. \n",
    "\n",
    "\n",
    "**Implement the Demographic Parity and Equality Odds metrics in the FairnessMetrics class similar to the Disparate Impact metric based on the formulation provided above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a3484-29d6-4efa-9434-c296450c49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairnessMetrics:\n",
    "    def __init__(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        sensitive_attribute: str, \n",
    "        pos_label: int, \n",
    "        true_label_col: str, \n",
    "        predicted_label_col: str, \n",
    "        privileged_groups: List[Dict[str, int]], \n",
    "        unprivileged_groups: List[Dict[str, int]]\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the FairnessMetrics class with dataset and attribute information.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            The dataframe containing the labels, predictions, and sensitive attribute.\n",
    "        \n",
    "        sensitive_attribute : str\n",
    "            The name of the sensitive attribute in the dataframe (e.g., race, gender).\n",
    "        \n",
    "        pos_label : int\n",
    "            The integer value representing a positive outcome in both the label and prediction columns.\n",
    "        \n",
    "        true_label_col : str\n",
    "            The name of the column representing the actual labels in the dataframe.\n",
    "        \n",
    "        predicted_label_col : str\n",
    "            The name of the column representing the predicted labels in the dataframe.\n",
    "        \n",
    "        privileged_groups : List[Dict[str, int]]\n",
    "            A list of dictionaries defining the privileged groups by attribute values.\n",
    "        \n",
    "        unprivileged_groups : List[Dict[str, int]]\n",
    "            A list of dictionaries defining the unprivileged groups by attribute values.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = df\n",
    "        self.sensitive_attribute = sensitive_attribute\n",
    "        self.pos_label = pos_label\n",
    "        self.true_label_col = true_label_col\n",
    "        self.predicted_label_col = predicted_label_col\n",
    "        self.privileged_groups = privileged_groups\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "\n",
    "    def compute_disparate_impact(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the disparate impact, a measure of fairness, for a specified sensitive attribute\n",
    "        based on positive prediction rates between privileged and unprivileged groups.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract privileged and unprivileged categories\n",
    "        privileged_categories = {group[self.sensitive_attribute] for group in self.privileged_groups}\n",
    "        unprivileged_categories = {group[self.sensitive_attribute] for group in self.unprivileged_groups}\n",
    "    \n",
    "        # Filter DataFrames for privileged and unprivileged groups\n",
    "        privileged_df = self.df[self.df[self.sensitive_attribute].isin(privileged_categories)]\n",
    "        unprivileged_df = self.df[self.df[self.sensitive_attribute].isin(unprivileged_categories)]\n",
    "        \n",
    "        # Calculate positive prediction rates\n",
    "        privileged_positive_rate = (\n",
    "            privileged_df[privileged_df[self.predicted_label_col] == pos_label].shape[0] / privileged_df.shape[0]\n",
    "        )\n",
    "        unprivileged_positive_rate = (\n",
    "            unprivileged_df[unprivileged_df[self.predicted_label_col] == pos_label].shape[0] / unprivileged_df.shape[0]\n",
    "        )\n",
    "    \n",
    "        # Avoid division by zero\n",
    "        if privileged_positive_rate == 0:\n",
    "            self.disparate_impact = float('inf')  # Undefined disparate impact when privileged group has zero positive rate\n",
    "            return\n",
    "    \n",
    "        # Compute disparate impact\n",
    "        self.disparate_impact = unprivileged_positive_rate / privileged_positive_rate\n",
    "\n",
    "    def compute_statistical_parity(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the statistical parity (or demographic parity), a measure of fairness, for a specified\n",
    "        sensitive attribute based on the difference in positive prediction rates between privileged and \n",
    "        unprivileged groups.\n",
    "        \"\"\"\n",
    "\n",
    "        # Write your code here\n",
    "        pass\n",
    "\n",
    "    def compute_equalized_odds(self) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Compute the equalized odds, a fairness measure that ensures both the true positive rates (TPRs) \n",
    "        and false positive rates (FPRs) are similar between privileged and unprivileged groups for a \n",
    "        specified sensitive attribute.\n",
    "        \"\"\"\n",
    "\n",
    "        # Write your code here\n",
    "        pass\n",
    "\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        self.compute_disparate_impact()\n",
    "        self.compute_statistical_parity()\n",
    "        self.compute_equalized_odds()\n",
    "\n",
    "        print(f\"Metadata:\")\n",
    "        print(f\"True Label Column: {self.true_label_col}\")\n",
    "        print(f\"Predicted Label Column: {self.predicted_label_col}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        print(\"Fairness Metrics:\")\n",
    "        print(f\"Disparate Impact: {self.disparate_impact:.3f}\")\n",
    "        print(f\"Demographic / Statistical Parity: {self.statistical_parity:.3f}\")\n",
    "        print(f\"Equalized Odds: TPR Difference = {self.tpr_difference:.3f}, FPR Difference: {self.fpr_difference:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40580c9-df1b-40c7-a99d-fb265d959c4f",
   "metadata": {},
   "source": [
    "### 3. Train a PyTorch MLP model to predict the income\n",
    "\n",
    "The predictor should take the features as input and output the probability of the income greater than 50K for that instance. Use BCELoss as the loss function. The model after the completion of training should have an accuracy score greater than 78%. Train the model on the training dataset `(X_train, y_train)` and evaluate the model's performance while training using the validation dataset `(X_test, y_test)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c6a14-8bc6-4b48-a581-7814402f9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df_encoded, train_size=0.7, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffadbf5-dec3-4376-9968-00b8a18ce482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "X_train_df = train.drop(columns=[\"income\"])\n",
    "y_train_df = train[\"income\"]\n",
    "\n",
    "X_test_df = test.drop(columns=[\"income\"])\n",
    "y_test_df = test[\"income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d88038-64fc-46e2-856f-9816d670e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "X_train = X_train_df.values\n",
    "y_train = y_train_df.values\n",
    "\n",
    "X_test = X_test_df.values\n",
    "y_test = y_test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83638ba7-d58f-447e-a5a8-c936ca73c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPTabular(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPTabular, self).__init__()\n",
    "\n",
    "        # Write your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Write your code here\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d703389-e60a-4f0e-8ff1-cf9b21ac7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1) \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c7c31-0314-4bbe-bbf1-0e9329a9b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # 1. Load the inputs and labels to the training device (CPU/GPU)\n",
    "        # Write your code here\n",
    "\n",
    "        # 2. Forward Pass and loss computation\n",
    "        # Write your code here\n",
    "\n",
    "        # 3. Backward Pass\n",
    "        # Write your code here\n",
    "\n",
    "        # Compute the loss and accuracy\n",
    "        # Write your code here\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def evaluate_model(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # 1. Load the inputs and labels to the training device (CPU/GPU)\n",
    "            # Write your code here\n",
    "    \n",
    "            # 2. Forward Pass and loss computation\n",
    "            # Write your code here\n",
    "\n",
    "            # 3. Compute the validation loss and accuracy\n",
    "            # Compute the loss and accuracy\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199fdb21-e2c2-4245-9892-9c56a982f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPTabular().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)\n",
    "    val_loss, val_accuracy = evaluate_model(model, criterion, val_loader, device)\n",
    "\n",
    "    if (epoch % 2) == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f} - \"\n",
    "              f\"Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5da0d-bd04-4694-9e05-acec0787b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    mlp_tabular_y_test_probs = model(X_test_tensor)\n",
    "\n",
    "test[\"mlp_tabular_preds\"] = np.where(mlp_tabular_y_test_probs >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be8c78-c6e9-4bfb-87a8-05da3b5de61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "mlp_clf_acc = accuracy_score(test[\"income\"], test[\"mlp_tabular_preds\"]) * 100\n",
    "print(f\"Accuracy of the MLP Classifier: {mlp_clf_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735d998-f5f3-4eff-8c93-53cbb25e3597",
   "metadata": {},
   "source": [
    "**Computing the Fairness metrics using the MLP model predictions for the `sex` sensitive attribute.**\n",
    "\n",
    "If we have a look at the value counts for the `sex` attribute. We can see that the Male category dominates the Female category. So, we hypothesize that the model would be biased towards the female category. The code cell below considers the male as the priviledged/dominant class and female as the unpriviledged class and finds the model bias using our fairness metric implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd57a6-3db7-459e-b5fb-4449e1171f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585add91-96ac-4932-a3de-6f0c4d00144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_attribute = 'sex'\n",
    "\n",
    "priviledged_groups = [{'sex': 0}]\n",
    "unpriviledged_groups = [{'sex': 1}]\n",
    "\n",
    "true_label_col = \"income\"\n",
    "predicted_label_col = \"mlp_tabular_preds\"\n",
    "\n",
    "pos_label = 1\n",
    "\n",
    "metrics = FairnessMetrics(\n",
    "    test, sensitive_attribute, pos_label, true_label_col, predicted_label_col,\n",
    "    priviledged_groups, unpriviledged_groups\n",
    ")\n",
    "\n",
    "metrics.compute_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d22ba1-cac4-43e3-be8c-028bfcf9ef96",
   "metadata": {},
   "source": [
    "**Similar to the above code, find the priviledged and unpriviledged categories for the `race` variable and compute the Fairness metrics using the MLP model predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49434c-6357-4ffa-9218-b9908bf4f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19684bc-4597-45e9-9b98-04bb7f58b972",
   "metadata": {},
   "source": [
    "**Similar to the above code, find the priviledged and unpriviledged categories for the `marital-status` variable and compute the Fairness metrics using the MLP model predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1989ea1-bbd8-4925-a69c-1892ca3f77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397b58e-01e7-432a-a535-dc416f95c0ef",
   "metadata": {},
   "source": [
    "### Introduction to Explainable AI (XAI)\n",
    "\n",
    "**Explainable AI (XAI)** is a field focused on making the decisions of machine learning models understandable and interpretable to humans. As AI and machine learning models are increasingly used to make critical decisions—in fields such as healthcare, finance, and justice—it’s essential to understand how these models arrive at their predictions or recommendations. Explainability helps build trust in AI systems, allows for error identification, and ensures that models are making fair and unbiased decisions.\n",
    "\n",
    "#### Why Linear and Tree-Based Models are Easier to Explain\n",
    "\n",
    "1. **Linear Models**: \n",
    "   - Linear models, such as linear regression or logistic regression, are straightforward because their predictions are based on a weighted sum of input features.\n",
    "   - The influence of each feature on the outcome can be easily understood by examining its weight: a positive weight indicates a positive influence, while a negative weight indicates a negative influence. This makes it easy to see which features drive the prediction and by how much.\n",
    "\n",
    "2. **Tree-Based Models**: \n",
    "   - Decision Trees are also interpretable because they make predictions through a series of if-then rules.\n",
    "   - Each decision node in the tree represents a split based on a feature, with branches leading to further splits or to leaf nodes (final predictions).\n",
    "   - Since the path from root to leaf reveals the steps taken to arrive at a prediction, it’s easy to trace and understand the reasoning behind each decision. This transparency makes it straightforward to explain predictions in terms of the conditions set by each split.\n",
    "\n",
    "These models are often considered **intrinsically interpretable** because their decision-making process aligns naturally with human reasoning.\n",
    "\n",
    "\n",
    "#### Why Black-Box Models are Difficult to Explain\n",
    "\n",
    "**Black-box models**, such as deep neural networks, ensemble models (like Random Forests), and support vector machines, are more complex and do not offer a straightforward interpretation of how individual features contribute to predictions. Here’s why they’re harder to explain:\n",
    "\n",
    "1. **Complex Internal Structure**:\n",
    "   - Black-box models can contain thousands or even millions of parameters that work together in complex ways to make predictions. For example, a deep neural network has multiple layers of interconnected neurons, each learning complex patterns from the data.\n",
    "   - Unlike linear models, there isn’t a simple, direct relationship between input features and the prediction, making it challenging to understand why the model made a particular decision.\n",
    "\n",
    "2. **Non-Linear Interactions**:\n",
    "   - Black-box models often capture non-linear relationships, where interactions between features influence the prediction in ways that can’t be easily decomposed or traced.\n",
    "   - In neural networks, for instance, inputs are transformed through a series of non-linear functions, and each layer adds more complexity, resulting in highly abstracted decision-making.\n",
    "\n",
    "3. **Opaque Decision-Making**:\n",
    "   - Black-box models don’t naturally reveal their decision-making processes. Unlike a decision tree that clearly shows the path taken to reach a decision, black-box models do not provide a clear, interpretable path from input to output.\n",
    "   - This opacity makes it difficult to identify which features had the most influence on a given prediction or to validate the fairness of the model.\n",
    "\n",
    "\n",
    "To make black-box models more interpretable, techniques like **SHAP (SHapley Additive exPlanations)** and **LIME (Local Interpretable Model-agnostic Explanations)** are used. These methods generate approximations of the model's behavior to provide insights into how different features contribute to individual predictions, but they do not offer the same level of transparency as linear or tree-based models.\n",
    "\n",
    "In summary, while linear and tree-based models are inherently interpretable due to their straightforward structure, black-box models are challenging to explain because of their complexity and lack of transparency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f8290-b785-47bf-962c-3fea9d5454ed",
   "metadata": {},
   "source": [
    "#### Shapley Values and the SHAP Python Library\n",
    "\n",
    "For this module, we will use the SHAP python libary to explain the reasonle behind our MLP model's predictions.\n",
    "\n",
    "Shapley values are a concept from cooperative game theory that provide a way to fairly distribute a payoff among players based on their contributions to the game. In the context of machine learning, Shapley values help us understand the contribution of each feature to a model’s predictions by treating each feature as a “player” in the game.\n",
    "\n",
    "Shapley values calculate the average contribution of each feature by considering every possible combination of features. For each prediction, they indicate how much each feature adds to or subtracts from the model’s baseline prediction. This approach provides a consistent and unbiased measure of feature importance, allowing us to see which features are driving individual predictions.\n",
    "\n",
    "Have you a look at this [Youtube Playlist](https://youtube.com/playlist?list=PLqDyyww9y-1SJgMw92x90qPYpHgahDLIK&si=--bNPcl3LCp2F25R) for learning about SHAP values in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d829271-f21c-4ffc-983c-3ebdb1562bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "# Shap takes considerable amount of time to run. It's better to run the Explainer on a smaller subset of our dataset.\n",
    "\n",
    "# `shap.kmeans` applies k-means clustering to the dataset to group data points into clusters based on their similarity.\n",
    "# Instead of using every data point to estimate Shapley values, shap.kmeans selects a small number of representative samples \n",
    "# from these clusters, reducing computational costs while maintaining accuracy in the Shapley value estimates.\n",
    "\n",
    "n_points = int(X_test.shape[0] * 0.01)\n",
    "X_test_sampled = shap.kmeans(X_test, k=n_points)\n",
    "\n",
    "# Instead of `shap.kmeans`, you can also randomly sample a subset of data points\n",
    "# idx = np.arange(X_test.shape[0])\n",
    "# random_idx = np.random.choice(idx, size=n_points)\n",
    "# X_test_sampled = X_test[random_idx]\n",
    "\n",
    "feature_names = test.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c157e6a-d3f5-404f-b1ea-ec78e1d18a69",
   "metadata": {},
   "source": [
    "**Use the GradientExplainer class from the Shap library to compute the feature importance of our MLP model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d080340-b81c-449b-887a-bfa7ee4c149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "shap.summary_plot(shap_values, feature_names, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315ddec-e04e-4071-adb0-ee9f8c6f8da1",
   "metadata": {},
   "source": [
    "**Point out your observations from the above plot. What features does the model uses for making the prediction. Based on the computed fairness metrics, do you see any concerns regarding the rationale behind the model's ability to make predictions?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d8cb1d0-5558-404f-960d-24444f3b9f48",
   "metadata": {},
   "source": [
    "### Adversarial Debiasing: A Method for Reducing Bias in Machine Learning Models\n",
    "\n",
    "In this section, you will implement **Adversarial Debiasing** to create a fair machine learning model. This approach leverages an **adversarial network** to prevent a model from picking up on and acting upon unwanted biases related to sensitive attributes like gender or race.\n",
    "\n",
    "### Adversarial Setup and Methodology\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./assets/adv_learning.png\" alt=\"centered image\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "This figure illustrates the **architecture of the adversarial network**, a core component of the **Adversarial Debiasing** methodology. In this architecture, a **Predictor** model works to produce accurate predictions, while an **Adversary** model attempts to detect any biases related to sensitive attributes. The combination of these two models forms a system designed to mitigate bias and promote fairer predictions.\n",
    "\n",
    "#### 1. Setting Up the Problem\n",
    "\n",
    "In typical machine learning tasks, a model (called the **Predictor**) is trained to predict an outcome $Y$ from input data $X$. For example, $Y$ might be an income category prediction based on data such as age, education, and hours worked. However, training data often contains biases related to sensitive features like gender or race. If left unchecked, these biases can lead the model to make unfair predictions.\n",
    "\n",
    "To mitigate this, **Adversarial Debiasing** introduces an additional model, called the **Adversary**, which works in opposition to the Predictor to identify and reduce the influence of sensitive features in the predictions.\n",
    "\n",
    "#### 2. Building a Predictor and an Adversary\n",
    "\n",
    "- **The Predictor**: This model’s goal is to predict the target outcome $\\hat{y}$ (e.g., income level) as accurately as possible. The Predictor has weights $W$, which are optimized to minimize **prediction loss** $L_P(\\hat{y}, y)$, where $y$ is the actual label.\n",
    "  \n",
    "- **The Adversary**: This model’s goal is to infer a protected attribute $\\hat{z}$ (e.g., gender or race) from the Predictor’s output $\\hat{y}$. The Adversary has weights $U$ and is trained to minimize **adversarial loss** $L_A(\\hat{z}, z)$, where $z$ is the true sensitive attribute. By detecting the sensitive attribute, the Adversary reveals any indirect biases present in the Predictor’s output.\n",
    "\n",
    "In essence, the Predictor attempts to produce accurate predictions while “hiding” any information about the sensitive attribute that the Adversary could use, making the output fairer and less biased.\n",
    "\n",
    "#### 3. Training with Competing Objectives\n",
    "\n",
    "During training:\n",
    "- The Predictor is updated to make accurate predictions about the target while minimizing any hint of the sensitive attribute. This is achieved by training it to both:\n",
    "  - Minimize prediction error (to be as accurate as possible).\n",
    "  - Minimize the Adversary’s ability to detect the sensitive attribute (e.g., gender) from its predictions.\n",
    "  \n",
    "- The Adversary, in turn, is updated to detect the sensitive attribute as accurately as possible, despite the Predictor’s efforts to obscure it.\n",
    "\n",
    "This dynamic creates a **“competition”** between the two models:\n",
    "- The **Predictor** seeks to minimize the Adversary’s success by avoiding reliance on sensitive features.\n",
    "- The **Adversary** strives to accurately detect the sensitive attribute, encouraging the Predictor to “hide” this information better.\n",
    "\n",
    "This competition helps ensure that the Predictor learns to make accurate predictions without using sensitive information, thus promoting fairness.\n",
    "\n",
    "#### 4. Achieving Fairness\n",
    "\n",
    "By balancing the training objectives of the Predictor and Adversary, the final Predictor model learns to make accurate predictions without relying on the sensitive attribute. This results in **fairer** predictions, as the sensitive attribute no longer influences the outcome.\n",
    "\n",
    "#### 5. Applying Fairness Metrics\n",
    "\n",
    "To assess the effectiveness of adversarial debiasing, we apply various fairness metrics, including:\n",
    "- **Demographic Parity**: Checks if positive prediction rates are similar across different groups.\n",
    "- **Equalized Odds**: Ensures that both false-positive rates and true-positive rates are balanced across groups.\n",
    "\n",
    "These metrics verify that the Predictor’s performance is consistent across different demographic groups and that adversarial training has effectively reduced bias.\n",
    "\n",
    "By the end of this implementation, your model should demonstrate reduced bias while maintaining strong predictive performance, achieving a balance between accuracy and fairness.\n",
    "\n",
    "Have a look at this [Youtube Video](https://youtu.be/37szRFkrmeQ?si=RMyBy-qJNHFoAI_F) to understand how Adversarial Debiasing works in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599c803-3ca8-4f1e-9ea1-09d1d3acaf2d",
   "metadata": {},
   "source": [
    "We'll implement AdversarialDebiasing using the Fairlearn package. Fairlearn is a libary that consists of pre-built fairness metrics and bias mitigation algorithms. References: [Ref 1](https://fairlearn.org/main/user_guide/mitigation/adversarial.html), [Ref 2](https://fairlearn.org/main/api_reference/generated/fairlearn.adversarial.AdversarialFairnessClassifier.html#fairlearn.adversarial.AdversarialFairnessClassifier), [Ref 3](https://fairlearn.org/main/auto_examples/plot_adversarial_basics.html#sphx-glr-auto-examples-plot-adversarial-basics-py), [Ref 4](https://fairlearn.org/main/auto_examples/plot_adversarial_fine_tuning.html#sphx-glr-auto-examples-plot-adversarial-fine-tuning-py)\n",
    "\n",
    "**Create an MLP Adversary model for performing Adversarial Debiasing and optimize for demographic parity on the marital-status sensitive variable. The model after adversarial training should have an accuracy more than 75, the disparate impact should be more than 0.4, the statistical parity should be less than 0.2, the TPR and FPR difference should be less than 0.1**\n",
    "\n",
    "Note: Check the documentation for the input and output shapes of the Adversary model. You may get incompatibility errors if you don't have them in the required size. Additionally, your model should also return logits instead of probability scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b5491-b46f-4474-98cc-c7693b77c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAdversary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPAdversary, self).__init__()\n",
    "        # Write your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Write your code here\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507824d-613e-4c68-a7e1-b47b65caafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MLPTabular()\n",
    "adversary = MLPAdversary()\n",
    "\n",
    "def optimizer_constructor(model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    return optimizer\n",
    "\n",
    "# Complete the AdversarialFairnessClassifier class to train a fair classifier\n",
    "adv_classifier = AdversarialFairnessClassifier(\n",
    "    backend=\"torch\",\n",
    "    # Write your code here\n",
    "    alpha=1,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef34a0b-3986-443d-890b-68b33263d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "Z = 'marital-status'\n",
    "\n",
    "Z_train = X_train_df[Z].values\n",
    "Z_test = X_test_df[Z].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088297a-1857-4629-b32f-6b7c6f0dd708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "adv_classifier.fit(X_train, y_train, sensitive_features=Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99ef1f-e764-47ef-be8a-fba325ad30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "y_test_adv_clf = adv_classifier.predict(X_test)\n",
    "test[\"adv_clf_preds\"] = y_test_adv_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbffe78-3801-4d3d-ab4c-ce45d4df13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't modify this code cell\n",
    "\n",
    "adv_clf_acc = accuracy_score(test[\"income\"], test[\"adv_clf_preds\"]) * 100\n",
    "print(f\"Accuracy of the Adversarial Classifier: {adv_clf_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf295e-6716-4098-9a98-8d928487efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the fairness metrics for the adversarial classifier. Check if the metrics have improved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c281910-cfda-4d14-98a6-44c6505eda50",
   "metadata": {},
   "source": [
    "**Now that you have debiased the model using Adversarial Debiasing, can you check the features that are important for the model for making the predictions using the KernelExplainer from Shap. Does the model work similar to your initial hypothesis about an optimal model $m^*$?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00157f9e-8e9f-450c-8ae3-9052c27254af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "shap.summary_plot(shap_values, feature_names, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f4732-66cb-4f76-b2be-6d105ceeb90e",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. UCI Machine Learning Repository: Adult Data Set. Available at: [https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult).\n",
    "\n",
    "2. Pessach, D. and Shmueli, E., 2022. A review on fairness in machine learning. *ACM Computing Surveys (CSUR)*, 55(3), pp.1-44.\n",
    "\n",
    "3. Xu, F., Uszkoreit, H., Du, Y., Fan, W., Zhao, D., & Zhu, J. (2019). Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges. In *Natural Language Processing and Chinese Computing* (pp. 563-574). Springer International Publishing, Cham.\n",
    "\n",
    "4. Zhang, B.H., Lemoine, B., and Mitchell, M., 2018, December. Mitigating unwanted biases with adversarial learning. In *Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society* (pp. 335-340).\n",
    "\n",
    "5. Kurakin, A., Goodfellow, I., and Bengio, S., 2016. Adversarial machine learning at scale. *arXiv preprint arXiv:1611.01236*.\n",
    "\n",
    "6. PyTorch Documentation. Available at: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "7. SHAP (SHapley Additive exPlanations) Documentation. Available at: [https://shap.readthedocs.io/en/latest/](https://shap.readthedocs.io/en/latest/).\n",
    "\n",
    "8. Achieving Algorithmic Fairness through Adversarial Training, Synapse 2022. Available at: [YouTube](https://www.youtube.com/watch?v=37szRFkrmeQ).\n",
    "\n",
    "9. Algorithm Fairness, A Data Odyssey. Available at: [YouTube](https://www.youtube.com/playlist?list=PLqDyyww9y-1Q0zWbng6vUOG1p3oReE2xS).\n",
    "\n",
    "10. SHAP Values, A Data Odyssey. Available at: [YouTube](https://www.youtube.com/playlist?list=PLqDyyww9y-1SJgMw92x90qPYpHgahDLIK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8016cd-ddd4-4f8e-b032-817c54b96b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
